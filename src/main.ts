// src/main.ts

import { getLlama, LlamaChatSession } from "node-llama-cpp";
import { searchHistory, HistoryItem } from "./searchHistory.js";
import fs from "fs";

const historyFilePath = "./history_content.json";

// ä»æ–‡ä»¶ä¸­è¯»å–å†å²è®°å½•
const historyItems: HistoryItem[] = JSON.parse(fs.readFileSync(historyFilePath, 'utf-8'));

const llama = await getLlama();
const model = await llama.loadModel({
    // modelPath: "./model/qwen1_5-1_8b-chat-q5_k_m.gguf",
    modelPath: "/Users/zoe/Documents/web3/ai-agent/zoe/eliza/agent/model.gguf",
});

console.log("ğŸ¤– æ¨¡å‹åŠ è½½å®Œæˆ");

export async function main(query: string) {
  const context = await model.createContext();
  const session = new LlamaChatSession({
    contextSequence: context.getSequence()
  });
  console.log("ğŸ¤– ä¼šè¯åˆ›å»ºå®Œæˆ");
  console.log("ğŸ¤– æ­£åœ¨æœç´¢å†å²è®°å½•...");

  const result = await searchHistory(query, historyItems, session);
  console.log("ğŸ¤– æœç´¢ç»“æœ:\n", result);
  return result;
}


// const query = "æˆ‘ä¹‹å‰çœ‹è¿‡å…³äº Web3 é’±åŒ…å®‰å…¨çš„æ–‡ç« ï¼Œæ˜¯å“ªä¸ªï¼Ÿ";
// main(query);